{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ccd3b2",
   "metadata": {},
   "source": [
    "# Generador de Haikus 3.0\n",
    "En este Notebook se explicará el tercer acercamiento a la creación de un generador de haikus realizado con Deep Learning. Intentaremos replicar el experimento explicado en el artículo: https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n",
    "\n",
    "Intentaremos lograr la generación de haikus a nivel de carácter haciendo uso de una red neuronal recurrente con una capa LSTM que reciba los tres versos como entrada simultáneamente, así como la longitud de los versos en sílabas.\n",
    "\n",
    "A continuación podemos ver todos los módulos que serán usados en el presente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
    "from keras.layers import Add, Dense, Input, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "import inflect\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885ea7c",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento\n",
    "En este bloque de código configuramos la ruta al dataset en nuestra máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infrared-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('')\n",
    "haiku_path = \"all_haiku.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e032964",
   "metadata": {},
   "source": [
    "Para este experimento necesitaremos contar el número de sílabas que tiene cada uno de los versos de los poemas. Para ello haremos uso del diccionario **CMU**, que podemos encontrar en http://www.speech.cs.cmu.edu/tools/lextool.html\n",
    "\n",
    "Este diccionario almacena un conjunto de fonemas que pueden contar como una sílaba para distintas palabras.\n",
    "\n",
    "También incluimos un fragmento de código para poder añadir palabras que el diccionario no tenga en cuenta, éstas deberán ser introducidas en el archivo \"*custom.dict.txt*\" siguiendo el formato del diccionario **CMU**.\n",
    "\n",
    "Para este experimento no haremos uso de ésta funcionalidad, solo haremos uso de las palabras predeterminadas del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ready-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los fonemas\n",
    "\n",
    "# Diccionario estándar\n",
    "WORDS = {}\n",
    "with (root_path / 'cmudict.dict.txt').open('r') as f:\n",
    "    for line in f.readlines():\n",
    "        word, phonemes = line.strip().split(' ', 1)\n",
    "        word = re.match(r'([^\\(\\)]*)(\\(\\d\\))*', word).groups()[0]\n",
    "        phonemes = phonemes.split(' ')\n",
    "        syllables = sum([re.match(r'.*\\d', p) is not None for p in phonemes])\n",
    "        #print(word, phonemes, syllables)\n",
    "        if word not in WORDS:\n",
    "            WORDS[word] = []\n",
    "        WORDS[word].append({\n",
    "            'phonemes': phonemes,\n",
    "            'syllables': syllables\n",
    "        })\n",
    "        \n",
    "CUSTOM_WORDS = {}\n",
    "vowels = ['AA', 'AE', 'AH', 'AO', 'AW', 'AX', 'AXR', 'AY', 'EH', 'ER', 'EY', 'IH', 'IX', 'IY', 'OW', 'OY', 'UH', 'UW', 'UX']\n",
    "with (root_path / 'custom.dict.txt').open('r') as f:\n",
    "    for line in f.readlines():\n",
    "        try:\n",
    "            word, phonemes = line.strip().split('\\t', 1)\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "        word = re.match(r'([^\\(\\)]*)(\\(\\d\\))*', word).groups()[0].lower()\n",
    "        phonemes = phonemes.split(' ')\n",
    "        syllables = sum([(p in vowels) for p in phonemes])\n",
    "        \n",
    "        if word not in CUSTOM_WORDS:\n",
    "            CUSTOM_WORDS[word] = []\n",
    "        CUSTOM_WORDS[word].append({\n",
    "            'phonemes': phonemes,\n",
    "            'syllables': syllables\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2fbbe",
   "metadata": {},
   "source": [
    "Una vez tenemos cargados los datos del diccionario de pronunciación, definiremos las funciones que leerán las líneas de los poemas, las limpiarán de símbolos y números y posteriormente contarán las sílabas que tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "million-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflect_engine = inflect.engine()\n",
    "\n",
    "# Diccionario de palabras no encontradas, deberán buscarse los fonemas\n",
    "NOT_FOUND = set()\n",
    "\n",
    "def get_words(line):\n",
    "    \n",
    "    # Creamos una lista con las palabras del verso\n",
    "    \n",
    "    line = str(line)\n",
    "    line = line.lower()\n",
    "    # Reemplaza las números con su palabra escrita normalmente\n",
    "    ws = []\n",
    "    for word in line.split(' '):\n",
    "        if re.search(r'\\d', word):\n",
    "            x = inflect_engine.number_to_words(word).replace('-', ' ')\n",
    "            ws = ws + x.split(' ')\n",
    "        else:\n",
    "            ws.append(word)\n",
    "\n",
    "    line = ' '.join(ws)\n",
    "\n",
    "    words = []\n",
    "    for word in line.split(' '):\n",
    "        word = re.match(r'[\\'\"]*([\\w\\']*)[\\'\"]*(.*)', word).groups()[0]\n",
    "        word = word.replace('_', '')\n",
    "        words.append(word)\n",
    "        \n",
    "    return words\n",
    "\n",
    "def count_non_standard_words(line):\n",
    "    \n",
    "    # Cuenta el número de palabras en el verso que no aparecen en el diccionario CMU.\n",
    "    \n",
    "    count = 0\n",
    "    for word in get_words(line):\n",
    "        if word and (word not in WORDS):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_syllable_count(line):\n",
    "    # Obtiene los posibles números de sílaba de la línea\n",
    "\n",
    "    counts = [0]\n",
    "    return_none = False\n",
    "    for word in get_words(line):\n",
    "        try:\n",
    "            if word:\n",
    "                if (word not in WORDS) and (word not in CUSTOM_WORDS):\n",
    "                    word = word.strip('\\'')\n",
    "                    \n",
    "                if word in WORDS:\n",
    "                    syllables = set(p['syllables'] for p in WORDS[word])\n",
    "                else:\n",
    "                    syllables = set(p['syllables'] for p in CUSTOM_WORDS[word])\n",
    "                #print(syllables)\n",
    "                new_counts = []\n",
    "                for c in counts:\n",
    "                    for s in syllables:\n",
    "                        new_counts.append(c+s)\n",
    "\n",
    "                counts = new_counts\n",
    "        except:\n",
    "            NOT_FOUND.add(word)\n",
    "            return_none = True\n",
    "\n",
    "    if return_none:\n",
    "        return None\n",
    "    \n",
    "    return ','.join([str(i) for i in set(counts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c29d46",
   "metadata": {},
   "source": [
    "Leeremos los poemas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brazilian-rochester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fishing boats</td>\n",
       "      <td>colors of</td>\n",
       "      <td>the rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ash wednesday--</td>\n",
       "      <td>trying to remember</td>\n",
       "      <td>my dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snowy morn--</td>\n",
       "      <td>pouring another cup</td>\n",
       "      <td>of black coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shortest day</td>\n",
       "      <td>flames dance</td>\n",
       "      <td>in the oven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haze</td>\n",
       "      <td>half the horse hidden</td>\n",
       "      <td>behind the house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144118</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144119</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144120</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144121</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144122</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144123 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0            fishing boats                         colors of   \n",
       "1          ash wednesday--               trying to remember    \n",
       "2             snowy morn--               pouring another cup   \n",
       "3             shortest day                      flames dance   \n",
       "4                     haze             half the horse hidden   \n",
       "...                    ...                               ...   \n",
       "144118  I'm not asking did            you say it nor clarify   \n",
       "144119     You are truly a               moron or a liar I'm   \n",
       "144120  Ain't no selfie on   this earth that's gonna make me   \n",
       "144121    is doing a great          job turning Independents   \n",
       "144122    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2  \n",
       "0                   the rainbow  \n",
       "1                      my dream  \n",
       "2               of black coffee  \n",
       "3                   in the oven  \n",
       "4              behind the house  \n",
       "...                         ...  \n",
       "144118    what you said neither  \n",
       "144119   inclined to think both  \n",
       "144120         like Theresa May  \n",
       "144121           into Democrats  \n",
       "144122  blood is loud Talk soon  \n",
       "\n",
       "[144123 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_haikus = pd.read_csv(str(haiku_path), usecols=[\"0\",\"1\",\"2\"])\n",
    "\n",
    "all_haikus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c3c92",
   "metadata": {},
   "source": [
    "Ahora aplicaremos las funciones que definimos anteriormente. Contaremos las sílabas de todos los versos y eliminaremos los poemas que tengan más de 3 palabras desconocidas.\n",
    "\n",
    "También almacenaremos en \"*unrecognized_words.txt*\" las palabras que no se han podido reconocer. Para mejorar el funcionamiento de este modelo podrían añadirse éstas palabras al diccionario personalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infrared-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras no reconocidas:  5796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>unknown_word_count</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fishing boats</td>\n",
       "      <td>colors of</td>\n",
       "      <td>the rainbow</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ash wednesday--</td>\n",
       "      <td>trying to remember</td>\n",
       "      <td>my dream</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5,6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snowy morn--</td>\n",
       "      <td>pouring another cup</td>\n",
       "      <td>of black coffee</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shortest day</td>\n",
       "      <td>flames dance</td>\n",
       "      <td>in the oven</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haze</td>\n",
       "      <td>half the horse hidden</td>\n",
       "      <td>behind the house</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144118</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144119</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144120</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144121</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144122</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142320 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0            fishing boats                         colors of   \n",
       "1          ash wednesday--               trying to remember    \n",
       "2             snowy morn--               pouring another cup   \n",
       "3             shortest day                      flames dance   \n",
       "4                     haze             half the horse hidden   \n",
       "...                    ...                               ...   \n",
       "144118  I'm not asking did            you say it nor clarify   \n",
       "144119     You are truly a               moron or a liar I'm   \n",
       "144120  Ain't no selfie on   this earth that's gonna make me   \n",
       "144121    is doing a great          job turning Independents   \n",
       "144122    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2  unknown_word_count 0_syllables 1_syllables  \\\n",
       "0                   the rainbow                   0           3           3   \n",
       "1                      my dream                   0           3         5,6   \n",
       "2               of black coffee                   0           3           6   \n",
       "3                   in the oven                   0           3           2   \n",
       "4              behind the house                   0           1           5   \n",
       "...                         ...                 ...         ...         ...   \n",
       "144118    what you said neither                   0           5           7   \n",
       "144119   inclined to think both                   0           5           7   \n",
       "144120         like Theresa May                   0           5           7   \n",
       "144121           into Democrats                   0           5           7   \n",
       "144122  blood is loud Talk soon                   0           5           7   \n",
       "\n",
       "       2_syllables  \n",
       "0                3  \n",
       "1                2  \n",
       "2                4  \n",
       "3                4  \n",
       "4                4  \n",
       "...            ...  \n",
       "144118           5  \n",
       "144119           5  \n",
       "144120           5  \n",
       "144121           5  \n",
       "144122           5  \n",
       "\n",
       "[142320 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Borra haikus con más de 3 palabras desconocidas\n",
    "all_haikus['unknown_word_count'] = np.sum([all_haikus[str(i)].apply(count_non_standard_words) for i in range(3)], axis=0)\n",
    "all_haikus = all_haikus[all_haikus['unknown_word_count'] < 3].copy()\n",
    "\n",
    "for i in range(3):\n",
    "    all_haikus['%s_syllables' % i] = all_haikus[str(i)].apply(get_syllable_count)\n",
    "    \n",
    "print(\"Palabras no reconocidas: \", len(NOT_FOUND))\n",
    "\n",
    "with open('unrecognized_words.txt', 'w') as f:\n",
    "    for w in NOT_FOUND:\n",
    "        f.write(w)\n",
    "        f.write('\\n')\n",
    "\n",
    "all_haikus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903604ae",
   "metadata": {},
   "source": [
    "Ahora haremos algo que no se lleva a cabo en el artículo. Debido a que nosotros tenemos un conjunto de haikus lo suficientemente grande, nos tomaremos la licencia de quedarnos solamente con los que tengan estructura 5-7-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "damaged-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el conjunto de poemas y nos quedamos con los que tengan estructura 5-7-5\n",
    "corpus = []\n",
    "all_haikus = all_haikus[all_haikus['0_syllables']==\"5\"]\n",
    "all_haikus = all_haikus[all_haikus['1_syllables']==\"7\"]\n",
    "all_haikus = all_haikus[all_haikus['2_syllables']==\"5\"]\n",
    "df = all_haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "steady-gates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['visiting the graves', 'profound blue of night ',\n",
       "       'scattered in the ditch ', ..., \"Ain't no selfie on\",\n",
       "       'is doing a great', 'Wanted to send a'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = df[['0', '1', '2']].values\n",
    "inputs[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c551195",
   "metadata": {},
   "source": [
    "Comenzamos a preparar los datos para el entrenamiento. Los inputs repetirán el primer carácter y los outputs se completarán añadiendo el próximo carácter, a excepción del último verso, que no lo hará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "chemical-moldova",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>unknown_word_count</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "      <th>0_in</th>\n",
       "      <th>0_out</th>\n",
       "      <th>1_in</th>\n",
       "      <th>1_out</th>\n",
       "      <th>2_in</th>\n",
       "      <th>2_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>visiting the graves</td>\n",
       "      <td>stronger the October wind</td>\n",
       "      <td>at my grandparents'</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>vvisiting the graves\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>visiting the graves\\ns\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>sstronger the October wind\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>stronger the October wind\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>aat my grandparents'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>at my grandparents'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>profound blue of night</td>\n",
       "      <td>the resin and salt of pines</td>\n",
       "      <td>so far from the sea</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>pprofound blue of night \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>profound blue of night \\nt\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>tthe resin and salt of pines\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>the resin and salt of pines\\ns\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>sso far from the sea\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>so far from the sea\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>scattered in the ditch</td>\n",
       "      <td>like tiny scraps of blue sky</td>\n",
       "      <td>bits of plastic bag</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>sscattered in the ditch \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>scattered in the ditch \\nl\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>llike tiny scraps of blue sky\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>like tiny scraps of blue sky\\nb\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>bbits of plastic bag\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bits of plastic bag\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>the smell of her hands</td>\n",
       "      <td>on the neck of the bottle</td>\n",
       "      <td>drinking greedily</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>tthe smell of her hands\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>the smell of her hands\\no\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>oon the neck of the bottle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>on the neck of the bottle\\nd\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>ddrinking greedily\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>drinking greedily\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>christmas services</td>\n",
       "      <td>a cellular phone rings out</td>\n",
       "      <td>handel's messiah</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>cchristmas services\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>christmas services\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>aa cellular phone rings out\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>a cellular phone rings out\\nh\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>hhandel's messiah\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>handel's messiah\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144118</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144119</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144120</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\n\\n\\n</td>\n",
       "      <td>this earth that's gonna make me\\nl\\n\\n</td>\n",
       "      <td>llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144121</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144122</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91749 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0                                 1  \\\n",
       "24          visiting the graves         stronger the October wind   \n",
       "141     profound blue of night        the resin and salt of pines   \n",
       "142     scattered in the ditch       like tiny scraps of blue sky   \n",
       "343      the smell of her hands         on the neck of the bottle   \n",
       "435          christmas services        a cellular phone rings out   \n",
       "...                         ...                               ...   \n",
       "144118       I'm not asking did            you say it nor clarify   \n",
       "144119          You are truly a               moron or a liar I'm   \n",
       "144120       Ain't no selfie on   this earth that's gonna make me   \n",
       "144121         is doing a great          job turning Independents   \n",
       "144122         Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2  unknown_word_count 0_syllables 1_syllables  \\\n",
       "24          at my grandparents'                   0           5           7   \n",
       "141         so far from the sea                   0           5           7   \n",
       "142         bits of plastic bag                   0           5           7   \n",
       "343           drinking greedily                   0           5           7   \n",
       "435            handel's messiah                   0           5           7   \n",
       "...                         ...                 ...         ...         ...   \n",
       "144118    what you said neither                   0           5           7   \n",
       "144119   inclined to think both                   0           5           7   \n",
       "144120         like Theresa May                   0           5           7   \n",
       "144121           into Democrats                   0           5           7   \n",
       "144122  blood is loud Talk soon                   0           5           7   \n",
       "\n",
       "       2_syllables                                               0_in  \\\n",
       "24               5  vvisiting the graves\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "141              5   pprofound blue of night \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "142              5   sscattered in the ditch \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "343              5  tthe smell of her hands\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "435              5  cchristmas services\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "...            ...                                                ...   \n",
       "144118           5  II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144119           5  YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "144120           5  AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144121           5  iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144122           5  WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    0_out  \\\n",
       "24      visiting the graves\\ns\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "141      profound blue of night \\nt\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "142      scattered in the ditch \\nl\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "343     the smell of her hands\\no\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "435     christmas services\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "...                                                   ...   \n",
       "144118  I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144119  You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "144120  Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144121  is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144122  Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     1_in  \\\n",
       "24         sstronger the October wind\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "141          tthe resin and salt of pines\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "142           llike tiny scraps of blue sky\\n\\n\\n\\n\\n\\n\\n   \n",
       "343        oon the neck of the bottle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "435         aa cellular phone rings out\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "...                                                   ...   \n",
       "144118     you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "144119    moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144120              this earth that's gonna make me\\n\\n\\n   \n",
       "144121       job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "144122        quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "\n",
       "                                                    1_out  \\\n",
       "24         stronger the October wind\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "141          the resin and salt of pines\\ns\\n\\n\\n\\n\\n\\n\\n   \n",
       "142           like tiny scraps of blue sky\\nb\\n\\n\\n\\n\\n\\n   \n",
       "343        on the neck of the bottle\\nd\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "435         a cellular phone rings out\\nh\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "...                                                   ...   \n",
       "144118    you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "144119   moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144120             this earth that's gonna make me\\nl\\n\\n   \n",
       "144121      job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "144122       quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "\n",
       "                                                     2_in  \\\n",
       "24      aat my grandparents'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "141     sso far from the sea\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "142     bbits of plastic bag\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "343     ddrinking greedily\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "435     hhandel's messiah\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "...                                                   ...   \n",
       "144118  wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "144119  iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "144120  llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144121  iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "144122   bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "\n",
       "                                                    2_out  \n",
       "24      at my grandparents'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "141     so far from the sea\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "142     bits of plastic bag\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "343     drinking greedily\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "435     handel's messiah\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "...                                                   ...  \n",
       "144118  what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "144119  inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "144120  like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "144121  into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "144122  blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \n",
       "\n",
       "[91749 rows x 13 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tomamos el máximo tamaño de línea de todos los versos y eliminamos las muestras\n",
    "# que sean más largas que el 99 percentil de longitud.\n",
    "\n",
    "max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n",
    "df = df[\n",
    "    (df['0'].str.len() <= max_line_length) & \n",
    "    (df['1'].str.len() <= max_line_length) & \n",
    "    (df['2'].str.len() <= max_line_length)\n",
    "].copy()\n",
    "df\n",
    "# Aplicamos padding a los versos que no tienen el máximo tamaño de línea\n",
    "# añadiendo \\n al final de los mismos hasta llegar a este tamaño máximo\n",
    "for i in range(3):\n",
    "    # Para los inputs, duplicaremos el primer caracter\n",
    "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "    # Añadimos el primer carácter de la próxima línea si no es el último verso\n",
    "    if i == 2: # Si es el último verso\n",
    "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
    "    else: # Si no es el último verso se añade el primer carácter de la próxima línea\n",
    "        # Esto ayudará con el entrenamiento, de manera que la siguiente RNR tenga mejores probabilidades de\n",
    "        # tomar el primer carácter correctamente.\n",
    "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "max_line_length += 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1d848",
   "metadata": {},
   "source": [
    "Ahora codificaremos los datos haciendo uso de codificación **one-hot** para que puedan ser utilizados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "inclusive-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[['0_in', '1_in', '2_in']].values\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(inputs.flatten())\n",
    "n_tokens = len(tokenizer.word_counts) + 1\n",
    "aux_X = [tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)]\n",
    "\n",
    "# X es la entrada de cada línea en secuencias de caracteres codificados en one-hot\n",
    "X = np_utils.to_categorical(np.array(aux_X), num_classes=n_tokens)\n",
    "\n",
    "outputs = df[['0_out', '1_out', '2_out']].values\n",
    "\n",
    "# Y es la entrada de cada línea en secuencias de caracteres codificados en one-hot\n",
    "Y = np_utils.to_categorical([tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)], num_classes=n_tokens)\n",
    "\n",
    "# X_syllables es la cuenta de sílabas de cada línea\n",
    "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa106d70",
   "metadata": {},
   "source": [
    "## 2. Creación del modelo\n",
    "Para este modelo, necesitaremos utilizar clases especiales haciendo uso de la API funcional de Keras. Estas clases constituirán las distintas líneas de entrenamiento que se usarán en la capa LSTM, de manera que puedan entrenarse simultáneamente.\n",
    "\n",
    "Crearemos una clase **TrainingLine** que constituirá una de las líneas de entrenamiento. Después definiremos una función *create_training_model* que creará las líneas de entrenamiento en función de los parámetros que consideremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "complex-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLine:\n",
    "    def __init__(self, name, previous_line, lstm, n_tokens):\n",
    "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
    "\n",
    "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
    "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
    "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
    "\n",
    "        #self.lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm_%s' % name)\n",
    "\n",
    "        #Si hay una linea previa, el estado inicial de ésta nueva será el que de como salida la anterior\n",
    "        if previous_line:\n",
    "            initial_state = [\n",
    "                Add(name='add_h_%s' % name)([\n",
    "                    previous_line.lstm_h,\n",
    "                    self.syllable_dense_output\n",
    "                ]),\n",
    "                Add(name='add_c_%s' % name)([\n",
    "                    previous_line.lstm_c,\n",
    "                    self.syllable_dense_output\n",
    "                ])\n",
    "            ]\n",
    "        else:\n",
    "            initial_state = [self.syllable_dense_output, self.syllable_dense_output]\n",
    "\n",
    "        self.lstm_out, self.lstm_h, self.lstm_c = lstm(self.char_input, initial_state=initial_state)\n",
    "\n",
    "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
    "        self.output = self.output_dense(self.lstm_out)\n",
    "\n",
    "def create_training_model(latent_dim, n_tokens):\n",
    "    lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm')\n",
    "    lines = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "#Se crean todas las lineas de inputs, que se anclan a la capa LSTM que creamos previamente\n",
    "    for i in range(3):\n",
    "        previous_line = lines[-1] if lines else None\n",
    "        lines.append(TrainingLine('line_%s' % i, previous_line, lstm, n_tokens))\n",
    "        inputs += [lines[-1].char_input, lines[-1].syllable_input]\n",
    "        outputs.append(lines[-1].output)\n",
    "\n",
    "    training_model = Model(inputs, outputs)\n",
    "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    return training_model, lstm, lines, inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e3a4e",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del modelo\n",
    "A continuación definiremos y entrenaremos el modelo haciendo uso de las clases definidas en el apartado anterior.\n",
    "\n",
    "La red consistirá en una capa LSTM de 2048 neuronas a la que se le pasarán 3 líneas de entrenamiento. En cada una de estas líneas se pasará un verso y las sílabas que tiene como estado. El número de sílabas, además, pasará por una capa Dense de tantas neuronas como tendrá la capa LSTM, con activación ***relu***. Por último, cada línea terminará con una capa Dense con tantas neuronas como caracteres se estén utilizando y función de activación ***softmax***.\n",
    "\n",
    "Entrenaremos la red durante 10 épocas con lotes de 256 muestras y reservando un 10% de las muestras para la validación. Monitorizaremos los resultados de cada época teniendo en cuenta la función de pérdida \"***categorical_crossentropy***\" y la precisión. Además, conforme pasen las épocas almacenaremos los pesos que mejores resultados hayan dado respecto a la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "indirect-conditioning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82574 samples, validate on 9175 samples\n",
      "Epoch 1/10\n",
      "82574/82574 [==============================] - 330s 4ms/step - loss: 5.6484 - output_line_0_loss: 1.5993 - output_line_1_loss: 2.3260 - output_line_2_loss: 1.7208 - output_line_0_acc: 0.5791 - output_line_1_acc: 0.3591 - output_line_2_acc: 0.5600 - val_loss: 3.8988 - val_output_line_0_loss: 1.0358 - val_output_line_1_loss: 1.7184 - val_output_line_2_loss: 1.1443 - val_output_line_0_acc: 0.6972 - val_output_line_1_acc: 0.4861 - val_output_line_2_acc: 0.6662\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.64845, saving model to output_salida\\2048-01-5.65-3.90.hdf5\n",
      "Epoch 2/10\n",
      "82574/82574 [==============================] - 314s 4ms/step - loss: 3.2841 - output_line_0_loss: 0.8550 - output_line_1_loss: 1.4744 - output_line_2_loss: 0.9541 - output_line_0_acc: 0.7432 - output_line_1_acc: 0.5519 - output_line_2_acc: 0.7114 - val_loss: 2.7855 - val_output_line_0_loss: 0.7119 - val_output_line_1_loss: 1.2789 - val_output_line_2_loss: 0.7946 - val_output_line_0_acc: 0.7823 - val_output_line_1_acc: 0.6063 - val_output_line_2_acc: 0.7572\n",
      "\n",
      "Epoch 00002: loss improved from 5.64845 to 3.28410, saving model to output_salida\\2048-02-3.28-2.79.hdf5\n",
      "Epoch 3/10\n",
      "82574/82574 [==============================] - 307s 4ms/step - loss: 2.5311 - output_line_0_loss: 0.6372 - output_line_1_loss: 1.1750 - output_line_2_loss: 0.7188 - output_line_0_acc: 0.8053 - output_line_1_acc: 0.6371 - output_line_2_acc: 0.7786 - val_loss: 2.3838 - val_output_line_0_loss: 0.5975 - val_output_line_1_loss: 1.1067 - val_output_line_2_loss: 0.6794 - val_output_line_0_acc: 0.8175 - val_output_line_1_acc: 0.6589 - val_output_line_2_acc: 0.7905\n",
      "\n",
      "Epoch 00003: loss improved from 3.28410 to 2.53112, saving model to output_salida\\2048-03-2.53-2.38.hdf5\n",
      "Epoch 4/10\n",
      "82574/82574 [==============================] - 303s 4ms/step - loss: 2.2446 - output_line_0_loss: 0.5612 - output_line_1_loss: 1.0557 - output_line_2_loss: 0.6276 - output_line_0_acc: 0.8266 - output_line_1_acc: 0.6732 - output_line_2_acc: 0.8054 - val_loss: 2.2236 - val_output_line_0_loss: 0.5616 - val_output_line_1_loss: 1.0401 - val_output_line_2_loss: 0.6218 - val_output_line_0_acc: 0.8243 - val_output_line_1_acc: 0.6769 - val_output_line_2_acc: 0.8079\n",
      "\n",
      "Epoch 00004: loss improved from 2.53112 to 2.24456, saving model to output_salida\\2048-04-2.24-2.22.hdf5\n",
      "Epoch 5/10\n",
      "82574/82574 [==============================] - 302s 4ms/step - loss: 2.0828 - output_line_0_loss: 0.5202 - output_line_1_loss: 0.9875 - output_line_2_loss: 0.5750 - output_line_0_acc: 0.8380 - output_line_1_acc: 0.6932 - output_line_2_acc: 0.8205 - val_loss: 2.1331 - val_output_line_0_loss: 0.5341 - val_output_line_1_loss: 1.0023 - val_output_line_2_loss: 0.5965 - val_output_line_0_acc: 0.8346 - val_output_line_1_acc: 0.6889 - val_output_line_2_acc: 0.8152\n",
      "\n",
      "Epoch 00005: loss improved from 2.24456 to 2.08283, saving model to output_salida\\2048-05-2.08-2.13.hdf5\n",
      "Epoch 6/10\n",
      "82574/82574 [==============================] - 302s 4ms/step - loss: 1.9617 - output_line_0_loss: 0.4917 - output_line_1_loss: 0.9356 - output_line_2_loss: 0.5343 - output_line_0_acc: 0.8458 - output_line_1_acc: 0.7084 - output_line_2_acc: 0.8327 - val_loss: 2.0900 - val_output_line_0_loss: 0.5235 - val_output_line_1_loss: 0.9834 - val_output_line_2_loss: 0.5830 - val_output_line_0_acc: 0.8376 - val_output_line_1_acc: 0.6967 - val_output_line_2_acc: 0.8195\n",
      "\n",
      "Epoch 00006: loss improved from 2.08283 to 1.96168, saving model to output_salida\\2048-06-1.96-2.09.hdf5\n",
      "Epoch 7/10\n",
      "82574/82574 [==============================] - 302s 4ms/step - loss: 1.8509 - output_line_0_loss: 0.4676 - output_line_1_loss: 0.8870 - output_line_2_loss: 0.4964 - output_line_0_acc: 0.8526 - output_line_1_acc: 0.7228 - output_line_2_acc: 0.8440 - val_loss: 2.1505 - val_output_line_0_loss: 0.5222 - val_output_line_1_loss: 1.0421 - val_output_line_2_loss: 0.5861 - val_output_line_0_acc: 0.8390 - val_output_line_1_acc: 0.6820 - val_output_line_2_acc: 0.8192\n",
      "\n",
      "Epoch 00007: loss improved from 1.96168 to 1.85089, saving model to output_salida\\2048-07-1.85-2.15.hdf5\n",
      "Epoch 8/10\n",
      "82574/82574 [==============================] - 304s 4ms/step - loss: 1.7404 - output_line_0_loss: 0.4454 - output_line_1_loss: 0.8375 - output_line_2_loss: 0.4576 - output_line_0_acc: 0.8591 - output_line_1_acc: 0.7377 - output_line_2_acc: 0.8559 - val_loss: 2.1039 - val_output_line_0_loss: 0.5235 - val_output_line_1_loss: 0.9892 - val_output_line_2_loss: 0.5910 - val_output_line_0_acc: 0.8392 - val_output_line_1_acc: 0.6979 - val_output_line_2_acc: 0.8201\n",
      "\n",
      "Epoch 00008: loss improved from 1.85089 to 1.74040, saving model to output_salida\\2048-08-1.74-2.10.hdf5\n",
      "Epoch 9/10\n",
      "82574/82574 [==============================] - 303s 4ms/step - loss: 1.6251 - output_line_0_loss: 0.4249 - output_line_1_loss: 0.7834 - output_line_2_loss: 0.4168 - output_line_0_acc: 0.8652 - output_line_1_acc: 0.7544 - output_line_2_acc: 0.8686 - val_loss: 2.1423 - val_output_line_0_loss: 0.5286 - val_output_line_1_loss: 1.0065 - val_output_line_2_loss: 0.6071 - val_output_line_0_acc: 0.8388 - val_output_line_1_acc: 0.6952 - val_output_line_2_acc: 0.8184\n",
      "\n",
      "Epoch 00009: loss improved from 1.74040 to 1.62513, saving model to output_salida\\2048-09-1.63-2.14.hdf5\n",
      "Epoch 10/10\n",
      "82574/82574 [==============================] - 302s 4ms/step - loss: 1.5078 - output_line_0_loss: 0.4064 - output_line_1_loss: 0.7267 - output_line_2_loss: 0.3748 - output_line_0_acc: 0.8710 - output_line_1_acc: 0.7722 - output_line_2_acc: 0.8821 - val_loss: 2.1954 - val_output_line_0_loss: 0.5338 - val_output_line_1_loss: 1.0313 - val_output_line_2_loss: 0.6300 - val_output_line_0_acc: 0.8386 - val_output_line_1_acc: 0.6941 - val_output_line_2_acc: 0.8163\n",
      "\n",
      "Epoch 00010: loss improved from 1.62513 to 1.50784, saving model to output_salida\\2048-10-1.51-2.20.hdf5\n"
     ]
    }
   ],
   "source": [
    "tf_session = tf.Session()\n",
    "K.set_session(tf_session)\n",
    "\n",
    "\n",
    "output_dir = Path('output_%s' % \"salida\")\n",
    "epochs = 10\n",
    "X_syllables = np.array(X_syllables)\n",
    "X_syllables = X_syllables.astype(np.float)\n",
    "training_model, lstm, lines, inputs, outputs = create_training_model(2048, n_tokens)\n",
    "\n",
    "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % 2048))\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]\n",
    "\n",
    "history = training_model.fit([\n",
    "    X[0], X_syllables[:,0], \n",
    "    X[1], X_syllables[:,1], \n",
    "    X[2], X_syllables[:,2]\n",
    "], [Y[0], Y[1], Y[2]], batch_size=256, epochs=epochs, validation_split=.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4288a7",
   "metadata": {},
   "source": [
    "Como podemos observar, los resultados son bastante buenos, con una precisión en la validación de alrededor del 80% en el primer verso y en el último.\n",
    "\n",
    "Sin embargo, el segundo verso obtiene alrededor de un 70% de precisión en la validación, algo que resulta llamativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723d910",
   "metadata": {},
   "source": [
    "## 5. Evaluación\n",
    "\n",
    "En esta ocasión trataremos de generar poemas con distintas temperaturas y observaremos la estructura que se construye en los mismos.\n",
    "\n",
    "Para esto, crearemos la función *sample*, que se encargará de predecir el próximo carácter en base a unas probabilidades y un valor de temperatura. \n",
    "\n",
    "También, en este caso, es necesario definir una clase ***GeneratorLine*** que represente las líneas de generador que se asocian a cada una de las ***TrainingLine*** del modelo.\n",
    "\n",
    "Crearemos una clase ***Generator*** que será la que utilicemos para definir nuestro generador. Esta clase tendrá un método ***generate_haiku*** que recibirá como parámetros un array con las sílabas que quisiéramos que tuviese el haiku, la temperatura a utilizar y el primer carácter utilizado.\n",
    "\n",
    "Para generar los haikus se ejecutarán algunos métodos en la sesión de TensorFlow tales como ***feed_dict*** para poder introducir los datos según se generen al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "altered-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.5):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "class GeneratorLine:\n",
    "    def __init__(self, name, training_line, lstm, n_tokens):\n",
    "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
    "\n",
    "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
    "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
    "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
    "\n",
    "        self.h_input = Input(shape=(lstm.units,), name='h_input_%s' % name)\n",
    "        self.c_input = Input(shape=(lstm.units,), name='c_input_%s' % name)\n",
    "        initial_state = [self.h_input, self.c_input]\n",
    "\n",
    "        self.lstm = lstm\n",
    "\n",
    "        self.lstm_out, self.lstm_h, self.lstm_c = self.lstm(self.char_input, initial_state=initial_state)\n",
    "\n",
    "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
    "        self.output = self.output_dense(self.lstm_out)\n",
    "\n",
    "        self.syllable_dense.set_weights(training_line.syllable_dense.get_weights())\n",
    "        #self.lstm.set_weights(lstm.get_weights())\n",
    "        self.output_dense.set_weights(training_line.output_dense.get_weights())\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, lstm, lines, tf_session, tokenizer, n_tokens, max_line_length):\n",
    "        self.tf_session = tf_session\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_tokens = n_tokens\n",
    "        self.max_line_length = max_line_length\n",
    "\n",
    "        self.lstm = LSTM(\n",
    "            lstm.units, return_state=True, return_sequences=True,\n",
    "            name='generator_lstm'\n",
    "        )\n",
    "        self.lines = [\n",
    "            GeneratorLine(\n",
    "                'generator_line_%s' % i,\n",
    "                lines[i], self.lstm, self.n_tokens\n",
    "            ) for i in range(3)\n",
    "        ]\n",
    "        self.lstm.set_weights(lstm.get_weights())\n",
    "\n",
    "    def generate_haiku(self, syllables=[5, 7, 5], temperature=.1, first_char=None):\n",
    "        output = []\n",
    "        h = None\n",
    "        c = None\n",
    "\n",
    "        if first_char is None:\n",
    "            first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n",
    "\n",
    "        next_char = self.tokenizer.texts_to_sequences(first_char)[0][0]\n",
    "\n",
    "        for i in range(3):\n",
    "            line = self.lines[i]\n",
    "            \n",
    "            s = self.tf_session.run(\n",
    "                line.syllable_dense_output,\n",
    "                feed_dict={\n",
    "                    line.syllable_input: [[syllables[i]]]\n",
    "                }\n",
    "            )\n",
    "            if h is None:\n",
    "                h = s\n",
    "                c = s\n",
    "            else:\n",
    "                h = h + s\n",
    "                c = c + s\n",
    "\n",
    "            line_output = [next_char]\n",
    "\n",
    "            end = False\n",
    "            next_char = None\n",
    "            for i in range(self.max_line_length):\n",
    "                char, h, c = self.tf_session.run(\n",
    "                    [line.output, line.lstm_h, line.lstm_c],\n",
    "                    feed_dict={\n",
    "                        line.char_input: [[\n",
    "                            np_utils.to_categorical(\n",
    "                                line_output[-1],\n",
    "                                num_classes=self.n_tokens\n",
    "                            )\n",
    "                        ]],\n",
    "                        line.h_input: h,\n",
    "                        line.c_input: c\n",
    "                    }\n",
    "                )\n",
    "                char = sample(char[0,0], temperature)\n",
    "                if char == 1 and not end:\n",
    "                    end = True\n",
    "                if char != 1 and end:\n",
    "                    next_char = char\n",
    "                    char = 1\n",
    "\n",
    "                line_output.append(char)\n",
    "\n",
    "            cleaned_text = self.tokenizer.sequences_to_texts([\n",
    "                line_output\n",
    "            ])[0].strip()[1:].replace(\n",
    "                '   ', '\\n'\n",
    "            ).replace(' ', '').replace('\\n', ' ')\n",
    "            #print(line_output)\n",
    "            print(cleaned_text)\n",
    "            output.append(cleaned_text)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be291ed2",
   "metadata": {},
   "source": [
    "Creamos un placeholder del modelo y cargamos los persos que deseemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2ed56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo placeholder para el modelo\n",
    "training_model, lstm, lines, inputs, outputs = create_training_model(2048, n_tokens)\n",
    "\n",
    "# Cargamos los pesos que queramos, especificando el archivo que los guardó\n",
    "training_model.load_weights(output_dir / '2048-10-1.51-2.20.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c15bd2",
   "metadata": {},
   "source": [
    "Creamos el generador con el modelo instanciado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "under-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6beec",
   "metadata": {},
   "source": [
    "A continuación podemos ver qué es lo que se genera al hacer uso del método ***generate_haiku***, mostrando los valores numéricos que se obtienen antes de ser traducidos a palabras y filtrados para que sean legibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "extraordinary-norway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 16, 8, 6, 4, 3, 13, 2, 4, 10, 3, 2, 18, 6, 8, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "united the wind\n",
      "[2, 2, 5, 20, 2, 4, 10, 3, 2, 13, 3, 24, 6, 12, 2, 22, 16, 4, 2, 6, 2, 13, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "f the devil but i do\n",
      "[8, 8, 5, 4, 2, 10, 7, 24, 3, 2, 4, 10, 3, 2, 4, 6, 14, 3, 2, 4, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "not have the time to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator.generate_haiku(temperature = 0.1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d758240",
   "metadata": {},
   "source": [
    "Generaremos 50 haikus con distintas temperaturas, siempre con el valor por defecto de sílabas 5-7-5. Confiamos en que esta estructura se mantendrá.\n",
    "\n",
    "En primer lugar se generarán 50 haikus con temperatura 0.1, después se generarán con temperatura 0.3 y, por último, con 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "logical-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally got a\n",
      "tory for the first time i\n",
      "got to stay home soon\n",
      "\n",
      "only the stream is\n",
      "he most beautiful person\n",
      "in the world to me\n",
      "\n",
      "unless you have to\n",
      "e the best thing ever i\n",
      "wish you could be fine\n",
      "\n",
      "keep your start because\n",
      "ou can't have to be the same\n",
      "as it is a word\n",
      "\n",
      "very sad sometimes\n",
      " stay in bed and watch the\n",
      "shit out of my head\n",
      "\n",
      "the more i listen\n",
      "o the world and then i get\n",
      "to keep my second\n",
      "\n",
      "question of the day\n",
      " have to get the best of\n",
      "my life and the rest\n",
      "\n",
      "question of the day\n",
      "he strength is the most stressful\n",
      "people in the world\n",
      "\n",
      "i wanna go to\n",
      "he gym but i don't want to\n",
      "be a good sense of\n",
      "\n",
      "when you realize\n",
      "he last time you want to be\n",
      "right now i love you\n",
      "\n",
      "kinda wanna go\n",
      "o the gym but i love the\n",
      "stream i love this shit\n",
      "\n",
      "finally got a\n",
      "ig bank on my face and i\n",
      "can't stand the shit out\n",
      "\n",
      "when you realize\n",
      "he last time i lost you and\n",
      "you can't have to see\n",
      "\n",
      "finally got a\n",
      "tory for the first time i\n",
      "got to see my screams\n",
      "\n",
      "u a woman with\n",
      "he super bowl i would have\n",
      "to see you good love\n",
      "\n",
      "have you ever smelled\n",
      " bad day i love you and\n",
      "you can't have to see\n",
      "\n",
      "very surprising\n",
      "han a bad day i wish i\n",
      "could be forever\n",
      "\n",
      "just because i stay\n",
      "he more i love you and i\n",
      "love you forever\n",
      "\n",
      "no matter how much\n",
      "ou try to see the stars of\n",
      "the rest of your life\n",
      "\n",
      "can someone tell me\n",
      "ow to do this and then i\n",
      "say it i miss it\n",
      "\n",
      "don't let your profile\n",
      "ic bitch and then they don't have\n",
      "to leave the power\n",
      "\n",
      "question of the sant\n",
      "n the studio who does\n",
      "not disrespect shit\n",
      "\n",
      "just because you should\n",
      "e stuck on stupid but the\n",
      "rest of your life so\n",
      "\n",
      "remember the last\n",
      "ime i lost the person who\n",
      "doesn't make me feel\n",
      "\n",
      "finally got a\n",
      "ig bank and i have to go\n",
      "out with the stream lol\n",
      "\n",
      "you can never sleep\n",
      "n the studio people\n",
      "should be in the world\n",
      "\n",
      "very sad sometimes\n",
      " love you and i love you\n",
      "and you can't handle\n",
      "\n",
      "unless it's too much\n",
      "or you to be the one that\n",
      "you love to see you\n",
      "\n",
      "just because they are\n",
      "o much better than they are\n",
      "not the same for you\n",
      "\n",
      "i wanna go to\n",
      "he gym but i wanna go\n",
      "home and call it too\n",
      "\n",
      "zerry christmas too\n",
      "y love you gotta be the\n",
      "best feeling ever\n",
      "\n",
      "my mom is someone\n",
      "ith me and my mom i can't\n",
      "stop smelling like that\n",
      "\n",
      "and the secret of\n",
      "he second time is the best\n",
      "shit i ever done\n",
      "\n",
      "have you ever smelled\n",
      " book or two of you and\n",
      "you can have the best\n",
      "\n",
      "zerry christmas my\n",
      "eart wont let me start to watch\n",
      "you think i love you\n",
      "\n",
      "just because you can't\n",
      "ait to get the best of you\n",
      "and then i love you\n",
      "\n",
      "everything is so\n",
      "un i love the most important\n",
      "thing i've ever heard\n",
      "\n",
      "can someone tell me\n",
      "ow to do this and then i\n",
      "say it i miss it\n",
      "\n",
      "x march is going\n",
      "o be a good day and i\n",
      "have to work on this\n",
      "\n",
      "people will always\n",
      "e the same as it they don't\n",
      "even love you too\n",
      "\n",
      "being a mom is\n",
      " good day and a half sit\n",
      "in the morning lol\n",
      "\n",
      "very sad about\n",
      "he stream and the strength to see\n",
      "any comic heart\n",
      "\n",
      "no matter how much\n",
      "ou try to do the same thing\n",
      "and still love you lol\n",
      "\n",
      "my mom really think\n",
      "hat we are going to be\n",
      "for a long long time\n",
      "\n",
      "can someone please tell\n",
      "e how to do the same thing\n",
      "about the stream comes\n",
      "\n",
      "everything is so\n",
      "un i love the most important\n",
      "thing i ever did\n",
      "\n",
      "very surprising\n",
      "han a big book and a bad\n",
      "bitch and the rest of\n",
      "\n",
      "good morning to all\n",
      "he best friends i hope the stream\n",
      "is a power move\n",
      "\n",
      "looks like a lovely\n",
      "erson who doesn't want to\n",
      "be a big mistake\n",
      "\n",
      "really want to go\n",
      "o the gym but i want to\n",
      "be a big mistake\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    generator.generate_haiku()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa57406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alright the screams i\n",
      "on't have to do the same thing\n",
      "about my day off\n",
      "\n",
      "people really tell\n",
      "e why they are not the same\n",
      "as it is to be\n",
      "\n",
      "my mom is going\n",
      "o be a good mom to see\n",
      "the next week of god\n",
      "\n",
      "x can't believe i\n",
      "ot to see the first time i\n",
      "got to see my screams\n",
      "\n",
      "keep your mind tonight\n",
      "hy do you see an instant\n",
      "sound like the last time\n",
      "\n",
      "me i'm so beauty\n",
      "or you now i can't wait to\n",
      "get a good combin\n",
      "\n",
      "x moment when you\n",
      "tart to find something that you\n",
      "can do to yourself\n",
      "\n",
      "good morning to all\n",
      "he boy start and change your life\n",
      "and it's the truth too\n",
      "\n",
      "life is a good mome\n",
      "the story is the lessons\n",
      "just to stay somewhere\n",
      "\n",
      "x may the person\n",
      "ho think they are not the same\n",
      "as you can do things\n",
      "\n",
      "change your moms another\n",
      "or you gotta stop the best\n",
      "personalities\n",
      "\n",
      "because the rest of\n",
      "ou want to be strong enough\n",
      "to be relevant\n",
      "\n",
      "how does anyone\n",
      "hink they are not the same as\n",
      "the people i love\n",
      "\n",
      "gotta get my nails\n",
      "o i can be so into\n",
      "the shower again\n",
      "\n",
      "question of the day\n",
      " have to be on the street\n",
      "and start to get it\n",
      "\n",
      "lil baby seems to\n",
      "e a good single for the\n",
      "festival story\n",
      "\n",
      "kinda wish i had\n",
      " baby it's like the light\n",
      "of my life so much\n",
      "\n",
      "really wish i could\n",
      "top being so bored for the stream\n",
      "i told you to me\n",
      "\n",
      "i wanna go to\n",
      "he gym but i wanna go\n",
      "home to my body\n",
      "\n",
      "remember when i\n",
      "ost my sister she looks like\n",
      "a good idea\n",
      "\n",
      "question of a train\n",
      "s the best feeling that you\n",
      "see me is the best\n",
      "\n",
      "lol i wanna go\n",
      "o the back of the season\n",
      "and not the stream lol\n",
      "\n",
      "question of the world\n",
      "s a personal season\n",
      "after a long day\n",
      "\n",
      "question of the day\n",
      " have a second time to\n",
      "get a picture of\n",
      "\n",
      "the amount of times\n",
      " start to find out what i\n",
      "want for my money\n",
      "\n",
      "video games are\n",
      "he same as i love the song\n",
      "before they see it\n",
      "\n",
      "been awake and changed\n",
      " lot of people in my\n",
      "life is so expensive\n",
      "\n",
      "you should be writing\n",
      "ike a bad ass hoe doesn't\n",
      "make you forget that\n",
      "\n",
      "remember the last\n",
      "ime i work today is the\n",
      "day i lost my shit\n",
      "\n",
      "really i want to\n",
      "o to the gym but i don't\n",
      "want to do this too\n",
      "\n",
      "zammer and the and\n",
      "ction is a good movie\n",
      "and i can't wait too\n",
      "\n",
      "can someone please tell\n",
      "e what the fuck is going\n",
      "on i love him so\n",
      "\n",
      "and the respect you\n",
      "hink is in the mood to be\n",
      "a suck i love you\n",
      "\n",
      "no one cares about\n",
      "e when i do the same thing\n",
      "about the struggle\n",
      "\n",
      "i just wanna be\n",
      "he best thing i ever do\n",
      "ever in my life\n",
      "\n",
      "x i'm so excited\n",
      "or you to be on a bitch\n",
      "and i love you all\n",
      "\n",
      "zamber and the stars\n",
      "f the story is a big\n",
      "man with a song time\n",
      "\n",
      "okay i got to\n",
      "top putting my single for\n",
      "the rest of my life\n",
      "\n",
      "sometimes i just say\n",
      "his the strength is going to\n",
      "be a good movie\n",
      "\n",
      "quick question of the\n",
      "rain and the stuff because of\n",
      "the people i love\n",
      "\n",
      "been waiting for the\n",
      "ay to get the best of me\n",
      "and i love that shit\n",
      "\n",
      "don't let your presence\n",
      "e a good self don't know how\n",
      "to be trusted there\n",
      "\n",
      "universe is so\n",
      "ucking much better than\n",
      "i was my name lol\n",
      "\n",
      "quick question does the\n",
      "tory is the second time\n",
      "to see in the sky\n",
      "\n",
      "my mom is sucking\n",
      "ack and i hate the shit i\n",
      "have in my life shit\n",
      "\n",
      "just say something that\n",
      "an happen to me i love\n",
      "you and my brother\n",
      "\n",
      "you can only get\n",
      " person they don't want to\n",
      "be with you with them\n",
      "\n",
      "i have a massage\n",
      "f my son and an alter\n",
      "and i love the show\n",
      "\n",
      "being an adult\n",
      "s so annoying and i\n",
      "have no idea\n",
      "\n",
      "kinda want to go\n",
      "o the gym with me i need\n",
      "a new case for me\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    generator.generate_haiku(temperature=.3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cff4054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question through this purk\n",
      "s cool are the mean says d\n",
      "everything for you\n",
      "\n",
      "mom birthday is much\n",
      "orse she might be fuck bitches\n",
      "all over igade\n",
      "\n",
      "virgil in overtime\n",
      "e is a roair she's me\n",
      "ryan can't sell out\n",
      "\n",
      "things a jecky need\n",
      "ungle dness is beautiful\n",
      "how long on the heart\n",
      "\n",
      "kenyrick should be\n",
      "orn homoreal corolat\n",
      "heartbreaking and clear\n",
      "\n",
      "you'll never know who\n",
      "othing you don't suppose to\n",
      "be eepent for you\n",
      "\n",
      "you gotta get up\n",
      "our club with the right pictures\n",
      "i just want it nace\n",
      "\n",
      "forgot to remain\n",
      "ife in the show guard angry\n",
      "sent the biation\n",
      "\n",
      "unlows in missing\n",
      "s an amazing show i\n",
      "need to update it\n",
      "\n",
      "don't look collactic\n",
      "nion email but for your\n",
      "movies is sy drunk\n",
      "\n",
      "like a ticket ass\n",
      "hotographer i feel like\n",
      "i'm in front of them\n",
      "\n",
      "leirn killing land with\n",
      "hat boam ast house needs to be\n",
      "greater than the legs\n",
      "\n",
      "zulled my bi boyhigh\n",
      "ut have four legend when we\n",
      "even bout to read\n",
      "\n",
      "people really just\n",
      "lear something once somebody\n",
      "sneeze me up lol\n",
      "\n",
      "going my hair done\n",
      " cant love my people are\n",
      "tea sisterg is not\n",
      "\n",
      "when you put a fac\n",
      " i got a few pissed on yours\n",
      "mandle or ratenja\n",
      "\n",
      "you meme wearing stless\n",
      "onight tenness evil make\n",
      "me myself happy\n",
      "\n",
      "question if you do\n",
      "rays like taking over not\n",
      "what makes you go through\n",
      "\n",
      "all i really do\n",
      "s let go how to get a\n",
      "baggo throwing things\n",
      "\n",
      "what's the premier league\n",
      "ou're novs and missing the next\n",
      "blast what a great work\n",
      "\n",
      "vegas today jess\n",
      "ight just sticking from her but\n",
      "she was a hick heart\n",
      "\n",
      "rube chanel versions\n",
      "look off an illngerant\n",
      "chanded for the real\n",
      "\n",
      "cheo adsect drave\n",
      "aid to a big botton to\n",
      "do any more looss\n",
      "\n",
      "u music as you\n",
      "onna have a nun inli\n",
      "you are the longest\n",
      "\n",
      "he means emails while\n",
      "hey know she'll beautiful him\n",
      "back for the best show\n",
      "\n",
      "finally did steam\n",
      "hanks for becoming a nor\n",
      "elentin tue day\n",
      "\n",
      "just look like honey\n",
      "e got to be pitced for\n",
      "everything entert\n",
      "\n",
      "me and multiphone\n",
      "nees this sound is down to earth\n",
      "or say that anymore\n",
      "\n",
      "the accent comes's\n",
      "ird not gonna be something\n",
      "so much to be you\n",
      "\n",
      "til the year bebask\n",
      "arastes are out to\n",
      "forcing this pro to\n",
      "\n",
      "discussing money\n",
      "verything you want nobody\n",
      "said you'd straight again\n",
      "\n",
      "it is a needed\n",
      "f something's dog comes on in\n",
      "january days\n",
      "\n",
      "just got a special\n",
      "all in love with the first args\n",
      "and mine i'm sorry\n",
      "\n",
      "i wan a watchest\n",
      "ire follower deal when i\n",
      "see something this fuck\n",
      "\n",
      "i'm going to choose\n",
      "ut izeal takes college then\n",
      "ahs do not do it\n",
      "\n",
      "zecoming tonight\n",
      "he semester isn't the\n",
      "based of where he was\n",
      "\n",
      "xto you ask you\n",
      "retty the world is becky\n",
      "and stupid ass doesn\n",
      "\n",
      "have a good offer\n",
      "or foy years when the book old\n",
      "bullshit is a ring\n",
      "\n",
      "never sit on my\n",
      "ew and a tenrolory\n",
      "to not im tired\n",
      "\n",
      "zoty of a long\n",
      "nd honestly they got a\n",
      "big today witchy\n",
      "\n",
      "romma journels spent\n",
      "ut my clicket next line but\n",
      "all you would have fors\n",
      "\n",
      "can't remember the\n",
      "oal light back redressed as this\n",
      "i lot watchyemed out\n",
      "\n",
      "letime thank you so\n",
      "alented me but i love\n",
      "you so nog your loss\n",
      "\n",
      "ok fest it rained\n",
      "ave a created pleasure\n",
      "for undone jones lol\n",
      "\n",
      "one on better with\n",
      "ne social people have you\n",
      "done you get it then\n",
      "\n",
      "nollaware so they\n",
      "ave so much suck it does lot\n",
      "make a soul hock there\n",
      "\n",
      "ugh lol illishous\n",
      "phurance of people\n",
      "they love clight a good\n",
      "\n",
      "crying my header\n",
      "earth in high school for a little\n",
      "but my dead longer\n",
      "\n",
      "no matter how much\n",
      "ou come through her is so much\n",
      "happy for him if\n",
      "\n",
      "that cough and spender\n",
      "ot sens if you're not at the\n",
      "saters it's two months\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    generator.generate_haiku(temperature=1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cdf91",
   "metadata": {},
   "source": [
    "Como podemos observar, sorprendentemente la mayoría de poemas mantienen el esquema 5-7-5, probablemente gracias a que se filtraron las muestras para que tuvieran esta estructura.\n",
    "\n",
    "Observamos que no se generan correctamente las primeras letras de la segunda línea en un gran número de ocasiones, pero esto no es un problema demasiado grave, ya que pueden intuirse con facilidad.\n",
    "\n",
    "Conforme aumenta la temperatura las palabras se vuelven más aleatorias e impredecibles, creando palabras que no existen en realidad pero que suenan creíbles. También, cuanto más se aumenta este valor menos sentido parecen tener los haikus.\n",
    "\n",
    "Podemos ver que la estructura, en su mayoría, sintácticamente tiene sentido. Sin embargo vemos que en muchas ocasiones los poemas no llegan a tener demasiado sentido, pero sí que podemos encontrar alguno que sorprende y parecería estar escrito por un humano.\n",
    "\n",
    "#### Haiku destacado 1\n",
    "\n",
    "question of the day\n",
    "\n",
    "I have to get the best of\n",
    "\n",
    "my life and the rest\n",
    "\n",
    "#### Haiku destacado 2\n",
    "\n",
    "just say something that\n",
    "\n",
    "can happen to me I love\n",
    "\n",
    "you and my brother\n",
    "\n",
    "#### Haiku destacado 3\n",
    "\n",
    "being an adult\n",
    "\n",
    "is so annoying and I\n",
    "\n",
    "have no idea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
